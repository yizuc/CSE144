{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GW3V2DJKjshq"
      },
      "source": [
        "# Outline\n",
        "\n",
        "* Today we will show how to train a ConvNet using PyTorch\n",
        "* We will also illustrate how the ConvNet makes use of specific assumptions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2YYOK2Yjshr"
      },
      "source": [
        "# To perform well, we need to incorporate some prior knowledge about the problem\n",
        "\n",
        "* Assumptions helps us when they are true\n",
        "* They hurt us when they are not\n",
        "* We want to make just the right amount of assumptions, not more than that\n",
        "\n",
        "## In Deep Learning\n",
        "\n",
        "* Many layers: compositionality\n",
        "* Convolutions: locality + stationarity of images\n",
        "* Pooling: Invariance of object class to translations"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/yizuc/CSE144\n",
        "import os\n",
        "os.chdir('/content/CSE144')"
      ],
      "metadata": {
        "id": "agfysQnmjuHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rmEVD_RRjshs"
      },
      "outputs": [],
      "source": [
        "from res.plot_lib import plot_data, plot_model, set_default"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSyLs0hqjshs"
      },
      "outputs": [],
      "source": [
        "set_default()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0XAo2H3jsht"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "\n",
        "# function to count number of parameters\n",
        "def get_n_params(model):\n",
        "    np=0\n",
        "    for p in list(model.parameters()):\n",
        "        np += p.nelement()\n",
        "    return np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jif49PVjsht"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8Gv0Pbsjsht"
      },
      "source": [
        "# Load the Dataset (MNIST)\n",
        "\n",
        "\n",
        "We can use some PyTorch DataLoader utilities for this. This will download, shuffle, normalize data and arrange it in batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmjY8yJtjsht"
      },
      "outputs": [],
      "source": [
        "input_size  = 28*28   # images are 28x28 pixels\n",
        "output_size = 10      # there are 10 classes\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=64, shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1307,), (0.3081,))\n",
        "                   ])),\n",
        "    batch_size=1000, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Up9aVaFHjshu"
      },
      "outputs": [],
      "source": [
        "# show some images\n",
        "plt.figure(figsize=(16, 6))\n",
        "for i in range(10):\n",
        "    plt.subplot(2, 5, i + 1)\n",
        "    image, _ = train_loader.dataset.__getitem__(i)\n",
        "    plt.imshow(image.squeeze().numpy())\n",
        "    plt.axis('off');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8qWMBcUjshu"
      },
      "source": [
        "# Create the model classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-fEFR-kjshu"
      },
      "outputs": [],
      "source": [
        "class FC2Layer(nn.Module):\n",
        "    def __init__(self, input_size, n_hidden, output_size):\n",
        "        super(FC2Layer, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(input_size, n_hidden), \n",
        "            nn.ReLU(), \n",
        "            nn.Linear(n_hidden, n_hidden), \n",
        "            nn.ReLU(), \n",
        "            nn.Linear(n_hidden, output_size), \n",
        "            nn.LogSoftmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, self.input_size)\n",
        "        return self.network(x)\n",
        "    \n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, input_size, n_feature, output_size):\n",
        "        super(CNN, self).__init__()\n",
        "        self.n_feature = n_feature\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=n_feature, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(n_feature, n_feature, kernel_size=5)\n",
        "        self.fc1 = nn.Linear(n_feature*4*4, 50)\n",
        "        self.fc2 = nn.Linear(50, 10)\n",
        "        \n",
        "    def forward(self, x, verbose=False):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, kernel_size=2)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, kernel_size=2)\n",
        "        x = x.view(-1, self.n_feature*4*4)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcRATXAHjshv"
      },
      "source": [
        "## Running on a GPU: device string\n",
        "\n",
        "Switching between CPU and GPU in PyTorch is controlled via a device string, which will seemlessly determine whether GPU is available, falling back to CPU if not:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5e2uQpydjshv"
      },
      "outputs": [],
      "source": [
        "accuracy_list = []\n",
        "\n",
        "def train(epochs, model, perm=torch.arange(0, 784).long()):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "      for batch_idx, (data, target) in enumerate(train_loader):\n",
        "          # send to device\n",
        "          data, target = data.to(device), target.to(device)\n",
        "          \n",
        "          # permute pixels\n",
        "          data = data.view(-1, 28*28)\n",
        "          data = data[:, perm]\n",
        "          data = data.view(-1, 1, 28, 28)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          output = model(data)\n",
        "          loss = F.nll_loss(output, target)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          if batch_idx % 100 == 0:\n",
        "              print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                  epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                  100. * batch_idx / len(train_loader), loss.item()))\n",
        "            \n",
        "def test(model, perm=torch.arange(0, 784).long()):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in test_loader:\n",
        "        # send to device\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        \n",
        "        # permute pixels\n",
        "        data = data.view(-1, 28*28)\n",
        "        data = data[:, perm]\n",
        "        data = data.view(-1, 1, 28, 28)\n",
        "        output = model(data)\n",
        "        test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss                                                               \n",
        "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability                                                                 \n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    accuracy = 100. * correct / len(test_loader.dataset)\n",
        "    accuracy_list.append(accuracy)\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        accuracy))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5KyxVdljshv"
      },
      "source": [
        "# Train a small fully-connected network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFvPcK-9jshw"
      },
      "outputs": [],
      "source": [
        "n_hidden = 8 # number of hidden units\n",
        "\n",
        "model_fnn = FC2Layer(input_size, n_hidden, output_size)\n",
        "model_fnn.to(device)\n",
        "\n",
        "optimizer = optim.SGD(model_fnn.parameters(), lr=0.01, momentum=0.5)\n",
        "print('Number of parameters: {}'.format(get_n_params(model_fnn)))\n",
        "\n",
        "train(epochs=1, model=model_fnn)\n",
        "test(model_fnn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEkpDF3fjshw"
      },
      "source": [
        "# Train a ConvNet with the same number of parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WvNg3LSjshw"
      },
      "outputs": [],
      "source": [
        "# Training settings \n",
        "n_features = 6 # number of feature maps\n",
        "\n",
        "model_cnn = CNN(input_size, n_features, output_size)\n",
        "model_cnn.to(device)\n",
        "\n",
        "optimizer = optim.SGD(model_cnn.parameters(), lr=0.01, momentum=0.5)\n",
        "print('Number of parameters: {}'.format(get_n_params(model_cnn)))\n",
        "\n",
        "train(epochs=1, model=model_cnn)\n",
        "test(model_cnn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uD5_H7z8jshw"
      },
      "source": [
        "# The ConvNet performs better with the same number of parameters, thanks to its use of prior knowledge about images\n",
        "\n",
        "* Use of convolution: Locality and stationarity in images\n",
        "* Pooling: builds in some translation invariance\n",
        "\n",
        "# What happens if the assumptions are no longer true?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apsPmz26jshw"
      },
      "outputs": [],
      "source": [
        "perm = torch.randperm(784)\n",
        "plt.figure(figsize=(16, 12))\n",
        "for i in range(10):\n",
        "    image, _ = train_loader.dataset.__getitem__(i)\n",
        "    # permute pixels\n",
        "    image_perm = image.view(-1, 28*28).clone()\n",
        "    image_perm = image_perm[:, perm]\n",
        "    image_perm = image_perm.view(-1, 1, 28, 28)\n",
        "    plt.subplot(4, 5, i + 1)\n",
        "    plt.imshow(image.squeeze().numpy())\n",
        "    plt.axis('off')\n",
        "    plt.subplot(4, 5, i + 11)\n",
        "    plt.imshow(image_perm.squeeze().numpy())\n",
        "    plt.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkRBOWdVjshw"
      },
      "source": [
        "# ConvNet with permuted pixels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3_Dv8zujshw"
      },
      "outputs": [],
      "source": [
        "# Training settings \n",
        "n_features = 6 # number of feature maps\n",
        "\n",
        "model_cnn = CNN(input_size, n_features, output_size)\n",
        "model_cnn.to(device)\n",
        "\n",
        "optimizer = optim.SGD(model_cnn.parameters(), lr=0.01, momentum=0.5)\n",
        "print('Number of parameters: {}'.format(get_n_params(model_cnn)))\n",
        "\n",
        "train(epochs=1, model=model_cnn, perm=perm)\n",
        "test(model=model_cnn, perm=perm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fp0kXD0Qjshx"
      },
      "source": [
        "# Fully-Connected with Permuted Pixels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfLUSOIPjshx"
      },
      "outputs": [],
      "source": [
        "n_hidden = 8    # number of hidden units\n",
        "\n",
        "model_fnn = FC2Layer(input_size, n_hidden, output_size)\n",
        "model_fnn.to(device)\n",
        "optimizer = optim.SGD(model_fnn.parameters(), lr=0.01, momentum=0.5)\n",
        "print('Number of parameters: {}'.format(get_n_params(model_fnn)))\n",
        "\n",
        "train(epochs=1, model=model_fnn, perm=perm)\n",
        "test(model=model_fnn, perm=perm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_BF-0iMjshx"
      },
      "source": [
        "# The ConvNet's performance drops when we permute the pixels, but the Fully-Connected Network's performance stays the same\n",
        "\n",
        "* ConvNet makes the assumption that pixels lie on a grid and are stationary/local\n",
        "* It loses performance when this assumption is wrong\n",
        "* The fully-connected network does not make this assumption\n",
        "* It does less well when it is true, since it doesn't take advantage of this prior knowledge\n",
        "* But it doesn't suffer when the assumption is wrong"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TcQR8FYBjshx"
      },
      "outputs": [],
      "source": [
        "plt.bar(('NN image', 'CNN image',\n",
        "         'CNN scrambled', 'NN scrambled'),\n",
        "        accuracy_list, width=0.4)\n",
        "plt.ylim((min(accuracy_list)-5, 96))\n",
        "plt.ylabel('Accuracy [%]')\n",
        "for tick in plt.gca().xaxis.get_major_ticks():\n",
        "    tick.label.set_fontsize(20)\n",
        "plt.title('Performance comparison');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SgiIB7Ejshx"
      },
      "source": [
        "### Checking Model Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVk2yqDkjshx"
      },
      "outputs": [],
      "source": [
        "print(dir(model_cnn))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8XLL4LMjshy"
      },
      "outputs": [],
      "source": [
        "print(dir(model_fnn))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}