{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJqyx9pWNPl7"
      },
      "source": [
        "# Transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAl-nA8SNUpr",
        "outputId": "42abc70a-acfb-4d19-ec2d-30c164b4a5e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'CSE144' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/yizuc/CSE144\n",
        "import os\n",
        "os.chdir('/content/CSE144')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1W5Hn0ZqUQ-1",
        "outputId": "e58ac61c-f67b-4a6d-be3f-f6ba81d5f8bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchtext==0.6.0 in /usr/local/lib/python3.10/dist-packages (0.6.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.27.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.0.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.22.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.16.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (0.1.99)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (3.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchtext==0.6.0) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchtext==0.6.0) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchtext==0.6.0) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchtext==0.6.0) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchtext==0.6.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "IJ-GK55RNPl8"
      },
      "outputs": [],
      "source": [
        "import torch \n",
        "from torch import nn\n",
        "import torch.nn.functional as f\n",
        "import numpy as np \n",
        "import pdb\n",
        "from torchtext import data, datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8RUzRbtpNPl9"
      },
      "outputs": [],
      "source": [
        "# Set the desired precision\n",
        "np.set_printoptions(precision=3, suppress=True)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "nn_Softargmax = nn.Softmax  # fix wrong name"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kU0k1DptNPl9"
      },
      "source": [
        "## Multi head attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKJzAB5L2wcA"
      },
      "source": [
        "Attention formula is implemented here:\n",
        "\n",
        "\\begin{aligned}\n",
        "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{{QK^T}}{\\sqrt{d_k}}\\right)V\n",
        "\\end{aligned}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "epFPNfUiNPl9"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_input=None):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads # head numbers\n",
        "        self.d_model = d_model # embedding dimension of model\n",
        "        if d_input is None:\n",
        "            d_xq = d_xk = d_xv = d_model\n",
        "        else:\n",
        "            d_xq, d_xk, d_xv = d_input\n",
        "            \n",
        "        # Make sure that the embedding dimension of model is a multiple of number of heads\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.d_k = d_model // self.num_heads\n",
        "        \n",
        "        # These are still of dimension d_model. They will be split into number of heads\n",
        "        # We are establishing the weights for query (q), key (k), and value (v) in this context.\n",
        "        self.W_q = nn.Linear(d_xq, d_model, bias=False)\n",
        "        self.W_k = nn.Linear(d_xk, d_model, bias=False)\n",
        "        self.W_v = nn.Linear(d_xv, d_model, bias=False)\n",
        "        \n",
        "        # Outputs of all sub-layers need to be of dimension d_model\n",
        "        self.W_h = nn.Linear(d_model, d_model)\n",
        "        \n",
        "    def scaled_dot_product_attention(self, Q, K, V):\n",
        "        # Q dimension: (bs, n_heads, q_length, dim_per_head)\n",
        "        # K dimension: (bs, n_heads, k_length, dim_per_head)\n",
        "        # V dimension: (bs, n_heads, v_length, dim_per_head)\n",
        "        batch_size = Q.size(0) \n",
        "        k_length = K.size(-2) \n",
        "        \n",
        "        # Scaling by d_k so that the soft(arg)max doesnt saturate\n",
        "        Q = Q / np.sqrt(self.d_k)                         # (bs, n_heads, q_length, dim_per_head)\n",
        "        scores = torch.matmul(Q, K.transpose(2,3))          # (bs, n_heads, q_length, k_length)\n",
        "        \n",
        "        # Calculate attention weights\n",
        "        A = nn_Softargmax(dim=-1)(scores)   # (bs, n_heads, q_length, k_length)\n",
        "        \n",
        "        # Get the weighted average of the values\n",
        "        \n",
        "        # Given our assumption of k_length = v_length,\n",
        "        # The multiplication can be expressed as: (bs, n_heads, q_length, k_length) * (bs, n_heads, v_length, dim_per_head)\n",
        "        # The resulting tensor will have dimensions (bs, n_heads, q_length, dim_per_head)\n",
        "        H = torch.matmul(A, V)     # (bs, n_heads, q_length, dim_per_head)\n",
        "        return H, A \n",
        "\n",
        "        \n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"\n",
        "        Split the last dimension (d_model) into (heads X depth)\n",
        "        Return after transpose to put in shape (batch_size X num_heads X seq_length X d_k)\n",
        "        \"\"\"\n",
        "        # -1 here is the seq_length\n",
        "        return x.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "    def group_heads(self, x, batch_size):\n",
        "        \"\"\"\n",
        "        Combine the heads again to get (batch_size X seq_length X (num_heads times d_k))\n",
        "        \"\"\"\n",
        "        # -1 here is the seq_length\n",
        "        # Important: d_model = self.num_heads * self.d_k\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, -1, self.num_heads * self.d_k)\n",
        "    \n",
        "\n",
        "    def forward(self, X_q, X_k, X_v):\n",
        "        batch_size, seq_length, dim = X_q.size()\n",
        "        # When applying X_q to the self.W_q layer, the shape remains unchanged: (bs, seq_length, dim). \n",
        "        # In PyTorch, the fully connected layer operates on the last dimension of the input tensor by default.\n",
        "\n",
        "        # After transforming, split into num_heads \n",
        "        Q = self.split_heads(self.W_q(X_q), batch_size)  # (bs, n_heads, q_length, dim_per_head)\n",
        "        K = self.split_heads(self.W_k(X_k), batch_size)  # (bs, n_heads, k_length, dim_per_head)\n",
        "        V = self.split_heads(self.W_v(X_v), batch_size)  # (bs, n_heads, v_length, dim_per_head)\n",
        "        \n",
        "        # Calculate the attention weights for each of the heads\n",
        "        H_cat, A = self.scaled_dot_product_attention(Q, K, V)\n",
        "        \n",
        "        # Put all the heads back together by concat\n",
        "        # dim here equals to d_model\n",
        "        H_cat = self.group_heads(H_cat, batch_size)    # (bs, q_length, dim)\n",
        "        \n",
        "        # Final linear layer  \n",
        "        # dim here equals to d_model\n",
        "        H = self.W_h(H_cat)          # (bs, q_length, dim)\n",
        "        \n",
        "        return H, A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H41Zx1Y8NPl-"
      },
      "source": [
        "### Some sanity checks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "nJzKDHQLNPl-"
      },
      "outputs": [],
      "source": [
        "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
        "def print_out(Q, K, V):\n",
        "    temp_out, temp_attn = temp_mha.scaled_dot_product_attention(Q, K, V)\n",
        "    print('Attention weights are:', temp_attn.squeeze().numpy())\n",
        "    print('Output is:', temp_out.squeeze().numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJFbHmFvNPl-"
      },
      "source": [
        "To check our self attention works - if the query matches with one of the key values, it should have all the attention focused there, with the value returned as the value at that index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81fDrMo4NPl_",
        "outputId": "07008819-502b-43f4-94e3-1240c92ccb52",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights are: [0. 1. 0. 0.]\n",
            "Output is: [10.004  0.     0.   ]\n"
          ]
        }
      ],
      "source": [
        "test_K = torch.tensor(\n",
        "    [[10, 0, 0],\n",
        "     [ 0,10, 0],\n",
        "     [ 0, 0,10],\n",
        "     [ 0, 0,10]]\n",
        ").float()[None,None]\n",
        "\n",
        "test_V = torch.tensor(\n",
        "    [[   1,0,0],\n",
        "     [  10,0,0],\n",
        "     [ 100,5,0],\n",
        "     [1000,6,0]]\n",
        ").float()[None,None]\n",
        "\n",
        "test_Q = torch.tensor(\n",
        "    [[0, 10, 0]]\n",
        ").float()[None,None]\n",
        "\n",
        "\n",
        "print_out(test_Q, test_K, test_V)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZilIGcU4cSyl",
        "outputId": "90da8f26-843c-4afb-addc-cc0ecb3ac647"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 4, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "test_K.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2WCSHZEcUPi",
        "outputId": "842e1c3b-c245-4610-c30d-d3abd87407c5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 1, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "test_Q.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMqCbbUZcWCZ",
        "outputId": "7a3272cc-3818-42ad-d6f6-24a40cc976f3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 4, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "test_V.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqwmWyAINPl_"
      },
      "source": [
        "Great! We can see that it focuses on the second key and returns the second value. \n",
        "\n",
        "If we give a query that matches two keys exactly, it should return the averaged value of the two values for those two keys. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4m8nG4IXNPl_",
        "outputId": "9041dc3f-dd63-4eb0-bba5-86d73603494d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights are: [0.  0.  0.5 0.5]\n",
            "Output is: [549.998   5.5     0.   ]\n"
          ]
        }
      ],
      "source": [
        "test_Q = torch.tensor([[0, 0, 10]]).float()  \n",
        "print_out(test_Q, test_K, test_V)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaWvDGbFNPmA"
      },
      "source": [
        "We see that it focuses equally on the third and fourth key and returns the average of their values.\n",
        "\n",
        "Now giving all the queries at the same time:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZkFP_r_NPmA",
        "outputId": "9ac23e40-de58-443f-9a2c-228b89237c67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights are: [[0.  0.  0.5 0.5]\n",
            " [0.  1.  0.  0. ]\n",
            " [0.5 0.5 0.  0. ]]\n",
            "Output is: [[549.998   5.5     0.   ]\n",
            " [ 10.004   0.      0.   ]\n",
            " [  5.502   0.      0.   ]]\n"
          ]
        }
      ],
      "source": [
        "test_Q = torch.tensor(\n",
        "    [[0, 0, 10], [0, 10, 0], [10, 10, 0]]\n",
        ").float()[None,None]\n",
        "print_out(test_Q, test_K, test_V)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezz4sI7INPmA"
      },
      "source": [
        "## 1D convolution with `kernel_size = 1`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKJtSF50NPmA"
      },
      "source": [
        "This is basically an MLP with one hidden layer and ReLU activation applied to each and every element in the set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "uOgr_kFMNPmA"
      },
      "outputs": [],
      "source": [
        "class FeedForwardLayer(nn.Module):\n",
        "    def __init__(self, d_model, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.k1convL1 = nn.Linear(d_model,    hidden_dim)\n",
        "        self.k1convL2 = nn.Linear(hidden_dim, d_model)\n",
        "        self.activation = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.k1convL1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.k1convL2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzeae73qNPmA"
      },
      "source": [
        "## Transformer encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_NQse7dNPmB"
      },
      "source": [
        "Now we have all components for our Transformer Encoder block shown below!!!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "AG9x-DcYNPmB"
      },
      "outputs": [],
      "source": [
        "# This is a single encoder layer that will be repeated multiple times later\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, conv_hidden_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffd = FeedForwardLayer(d_model, conv_hidden_dim)\n",
        "\n",
        "        self.layernorm1 = nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
        "        self.layernorm2 = nn.LayerNorm(normalized_shape=d_model, eps=1e-6)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        \n",
        "        # Multi-head attention\n",
        "        # In this case, Q, K, and V are identical and correspond to the same input, which is often referred to as self-attention.\n",
        "        attn_output, _ = self.mha(x, x, x)  # (batch_size, input_seq_len, d_model)\n",
        "        \n",
        "        # Layer norm after adding the residual connection \n",
        "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "        \n",
        "        # Feed forward \n",
        "        ffd_output = self.ffd(out1)  # (batch_size, input_seq_len, d_model)\n",
        "        \n",
        "        #Second layer norm after adding residual connection \n",
        "        out2 = self.layernorm2(out1 + ffd_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        return out2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5D3Vm23NPmB"
      },
      "source": [
        "### Encoder \n",
        "#### Blocks of N Encoder Layers + Positional encoding + Input embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRxgZQPCNPmB"
      },
      "source": [
        "Self attention by itself does not have any recurrence or convolutions so to make it sensitive to position we must provide additional positional encodings. These are calculated as follows:\n",
        "\n",
        "\\begin{aligned}\n",
        "E(p, 2i)    &= \\sin(p / 10000^{2i / d}) \\\\\n",
        "E(p, 2i+1) &= \\cos(p / 10000^{2i / d})\n",
        "\\end{aligned}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "iH2zUcK0NPmB"
      },
      "outputs": [],
      "source": [
        "def create_sinusoidal_embeddings(nb_p, dim, E):\n",
        "    theta = np.array([\n",
        "        [p / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)]\n",
        "        for p in range(nb_p)\n",
        "    ])\n",
        "\n",
        "    # Fix: move requires_grad here to avoid the error: RuntimeError: a view of a leaf Variable that requires grad is being used in an in-place operation.\n",
        "    E.requires_grad = False\n",
        "    E[:, 0::2] = torch.FloatTensor(np.sin(theta[:, 0::2]))\n",
        "    E[:, 1::2] = torch.FloatTensor(np.cos(theta[:, 1::2]))\n",
        "    E = E.to(device)\n",
        "    \n",
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, d_model, vocab_size, max_position_embeddings):\n",
        "        super().__init__()\n",
        "        # By setting padding_idx=1, it means that the index 1 in the vocabulary is reserved for the padding token.\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, d_model, padding_idx=1)\n",
        "        self.position_embeddings = nn.Embedding(max_position_embeddings, d_model)\n",
        "        # The self.position_embeddings.weight tensor is assigned values within the create_sinusoidal_embeddings function.\n",
        "        create_sinusoidal_embeddings(\n",
        "            nb_p=max_position_embeddings,\n",
        "            dim=d_model,\n",
        "            E=self.position_embeddings.weight\n",
        "        )\n",
        "\n",
        "        self.LayerNorm = nn.LayerNorm(d_model, eps=1e-12)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        seq_length = input_ids.size(1)\n",
        "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device) # (max_seq_length)\n",
        "        # Repeat position_ids for batchsize (bs) times\n",
        "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)                      # (bs, max_seq_length)\n",
        "        \n",
        "        # Get word embeddings for each input id\n",
        "        word_embeddings = self.word_embeddings(input_ids)                   # (bs, max_seq_length, dim)\n",
        "        \n",
        "        # Get position embeddings for each position id \n",
        "        position_embeddings = self.position_embeddings(position_ids)        # (bs, max_seq_length, dim)\n",
        "        \n",
        "        # Add them both \n",
        "        embeddings = word_embeddings + position_embeddings  # (bs, max_seq_length, dim)\n",
        "        \n",
        "        # Layer norm \n",
        "        embeddings = self.LayerNorm(embeddings)             # (bs, max_seq_length, dim)\n",
        "        return embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ncmixAVZNPmB"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, num_layers, d_model, num_heads, ff_hidden_dim, input_vocab_size,\n",
        "               maximum_position_encoding):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = Embeddings(d_model, input_vocab_size,maximum_position_encoding)\n",
        "\n",
        "        # `nn.ModuleList` is a container that holds multiple PyTorch modules, \n",
        "        # allowing for easier management and efficient processing of the modules within a neural network.\n",
        "        self.enc_layers = nn.ModuleList()\n",
        "        for _ in range(num_layers):\n",
        "            self.enc_layers.append(EncoderLayer(d_model, num_heads, ff_hidden_dim))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x) # Transform to (batch_size, input_seq_length, d_model)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x)\n",
        "\n",
        "        return x  # (batch_size, input_seq_len, d_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKSwVUtk3SWB"
      },
      "source": [
        "### Prepare for dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dicBVTUNPmC",
        "outputId": "2b949dee-5cd9-48d6-87df-2f790230fbf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train :  25000\n",
            "test :  25000\n",
            "train.fields : {'text': <torchtext.data.field.Field object at 0x7f4ae2c2dd20>, 'label': <torchtext.data.field.LabelField object at 0x7f4ae2c2cd60>}\n"
          ]
        }
      ],
      "source": [
        "max_len = 200\n",
        "text = data.Field(sequential=True, fix_length=max_len, batch_first=True, lower=True, dtype=torch.long)\n",
        "label = data.LabelField(sequential=False, dtype=torch.long)\n",
        "ds_train, ds_test = datasets.IMDB.splits(text, label, root='./')\n",
        "print('train : ', len(ds_train))\n",
        "print('test : ', len(ds_test))\n",
        "print('train.fields :', ds_train.fields)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "JNWSeoUtNPmC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5ba3e19-92ef-418d-95e4-b5c7be42679e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train :  22500\n",
            "valid :  2500\n",
            "test :  25000\n"
          ]
        }
      ],
      "source": [
        "ds_train, ds_valid = ds_train.split(0.9)\n",
        "print('train : ', len(ds_train))\n",
        "print('valid : ', len(ds_valid))\n",
        "print('test : ', len(ds_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "lvmihRl-NPmC"
      },
      "outputs": [],
      "source": [
        "num_words = 50_000\n",
        "text.build_vocab(ds_train, max_size=num_words)\n",
        "label.build_vocab(ds_train)\n",
        "vocab = text.vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yb0K__JW0yfh"
      },
      "source": [
        "If you remember, when we define word embedding layer, we set padding_idx to 1\n",
        "```\n",
        "self.word_embeddings = nn.Embedding(vocab_size, d_model, padding_idx=1)\n",
        "```\n",
        "That's because in vocab dictionary, the word at index 1 is <*pad*>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ugZqm0i0Fgv",
        "outputId": "e202c7d5-a6b1-438c-99e8-2805fedba0b1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<unk>', '<pad>', 'the', 'a', 'and', 'of', 'to', 'is', 'in', 'i']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "vocab.itos[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "SNcg-FTSNPmC"
      },
      "outputs": [],
      "source": [
        "batch_size = 164\n",
        "train_loader, valid_loader, test_loader = data.BucketIterator.splits(\n",
        "    (ds_train, ds_valid, ds_test), batch_size=batch_size, sort_key=lambda x: len(x.text), repeat=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GQW-l513dY0"
      },
      "source": [
        "### Define and train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "xFNI1ICQNPmC"
      },
      "outputs": [],
      "source": [
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self, num_layers, d_model, num_heads, conv_hidden_dim, input_vocab_size, num_answers):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = Encoder(num_layers, d_model, num_heads, conv_hidden_dim, input_vocab_size,\n",
        "                         maximum_position_encoding=10000)\n",
        "        self.dense = nn.Linear(d_model, num_answers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        # x shape here is [164, 200, 32]\n",
        "        # Here we are extracting the maximum value from each sequence of length 200.\n",
        "        x, _ = torch.max(x, dim=1)\n",
        "        # Resulted x has size [164, 32]\n",
        "        x = self.dense(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxJTCwibNPmD",
        "outputId": "d50a606a-681b-4c08-d13a-7e703a2ca064"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TransformerClassifier(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embeddings(\n",
              "      (word_embeddings): Embedding(50002, 32, padding_idx=1)\n",
              "      (position_embeddings): Embedding(10000, 32)\n",
              "      (LayerNorm): LayerNorm((32,), eps=1e-12, elementwise_affine=True)\n",
              "    )\n",
              "    (enc_layers): ModuleList(\n",
              "      (0): EncoderLayer(\n",
              "        (mha): MultiHeadAttention(\n",
              "          (W_q): Linear(in_features=32, out_features=32, bias=False)\n",
              "          (W_k): Linear(in_features=32, out_features=32, bias=False)\n",
              "          (W_v): Linear(in_features=32, out_features=32, bias=False)\n",
              "          (W_h): Linear(in_features=32, out_features=32, bias=True)\n",
              "        )\n",
              "        (ffd): FeedForwardLayer(\n",
              "          (k1convL1): Linear(in_features=32, out_features=128, bias=True)\n",
              "          (k1convL2): Linear(in_features=128, out_features=32, bias=True)\n",
              "          (activation): ReLU()\n",
              "        )\n",
              "        (layernorm1): LayerNorm((32,), eps=1e-06, elementwise_affine=True)\n",
              "        (layernorm2): LayerNorm((32,), eps=1e-06, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dense): Linear(in_features=32, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "model = TransformerClassifier(num_layers=1, d_model=32, num_heads=2, \n",
        "                         conv_hidden_dim=128, input_vocab_size=50002, num_answers=2)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "F47h01LGNPmD"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
        "epochs = 10\n",
        "t_total = len(train_loader) * epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "XrPECWBaNPmD"
      },
      "outputs": [],
      "source": [
        "def train(train_loader, valid_loader):\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        train_iterator, valid_iterator = iter(train_loader), iter(valid_loader)\n",
        "        nb_batches_train = len(train_loader)\n",
        "        train_acc = 0\n",
        "        model.train()\n",
        "        losses = 0.0\n",
        "\n",
        "        for batch in train_iterator:\n",
        "            x = batch.text.to(device)\n",
        "            y = batch.label.to(device)\n",
        "            \n",
        "            out = model(x)\n",
        "\n",
        "            loss = f.cross_entropy(out, y)\n",
        "            \n",
        "            model.zero_grad()\n",
        "\n",
        "            loss.backward()\n",
        "            losses += loss.item()\n",
        "\n",
        "            optimizer.step()\n",
        "                        \n",
        "            train_acc += (out.argmax(1) == y).cpu().numpy().mean()\n",
        "        \n",
        "        print(f\"Training loss at epoch {epoch} is {losses / nb_batches_train}\")\n",
        "        print(f\"Training accuracy: {train_acc / nb_batches_train}\")\n",
        "        print('Evaluating on validation:')\n",
        "        evaluate(valid_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "NToyoIZ1NPmD"
      },
      "outputs": [],
      "source": [
        "def evaluate(data_loader):\n",
        "    data_iterator = iter(data_loader)\n",
        "    nb_batches = len(data_loader)\n",
        "    model.eval()\n",
        "    acc = 0 \n",
        "    for batch in data_iterator:\n",
        "        x = batch.text.to(device)\n",
        "        y = batch.label.to(device)\n",
        "                \n",
        "        out = model(x)\n",
        "        acc += (out.argmax(1) == y).cpu().numpy().mean()\n",
        "\n",
        "    print(f\"Eval accuracy: {acc / nb_batches}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "6gIPZMOUNPmD",
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46db36b4-e202-4997-c489-3cdc3d47ac20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss at epoch 0 is 0.7130292183247166\n",
            "Training accuracy: 0.5481121862849063\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.609641768292683\n",
            "Training loss at epoch 1 is 0.6319503183814063\n",
            "Training accuracy: 0.6560953517143865\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.6912728658536585\n",
            "Training loss at epoch 2 is 0.5598461429278055\n",
            "Training accuracy: 0.7220970307529163\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.7504192073170733\n",
            "Training loss at epoch 3 is 0.46444259324799414\n",
            "Training accuracy: 0.7882644043831748\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.7946646341463415\n",
            "Training loss at epoch 4 is 0.38757054745287134\n",
            "Training accuracy: 0.8291246907034285\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.798170731707317\n",
            "Training loss at epoch 5 is 0.32030327715303586\n",
            "Training accuracy: 0.865257158006363\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.81875\n",
            "Training loss at epoch 6 is 0.2697371229313422\n",
            "Training accuracy: 0.8914755655708729\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.827934451219512\n",
            "Training loss at epoch 7 is 0.22284039325904156\n",
            "Training accuracy: 0.9158050547896786\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.823704268292683\n",
            "Training loss at epoch 8 is 0.18185384711925534\n",
            "Training accuracy: 0.9351747525627431\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.8317454268292682\n",
            "Training loss at epoch 9 is 0.14569787189796352\n",
            "Training accuracy: 0.9507334747260515\n",
            "Evaluating on validation:\n",
            "Eval accuracy: 0.8283536585365856\n"
          ]
        }
      ],
      "source": [
        "train(train_loader, valid_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "z81jwY-mNPmD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b070d056-3d7d-4b3f-b7cc-bc7c50240e66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eval accuracy: 0.8098309332766533\n"
          ]
        }
      ],
      "source": [
        "evaluate(test_loader)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 [conda env:pDL]",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}