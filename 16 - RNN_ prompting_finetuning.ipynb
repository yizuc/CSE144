{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n94fTFdhz4lI"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This exercise assignment aims to provide hands-on experience with three different approaches in natural language processing: RNN model training, prompting a pretrained language model, and fine-tuning a language model. The task is to classify emotions in text using the Emotion dataset available at Hugging Face's Emotion Dataset.\n",
        "\n",
        "https://huggingface.co/datasets/dair-ai/emotion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naEFoNh83umj"
      },
      "source": [
        "## Data set\n",
        "\n",
        "* Utilize the Emotion dataset from Hugging Face.\n",
        "* You will apply three approaches to classify emotions such as sadness, joy, love, anger, fear, and surprise from textual data.\n",
        "* More details about the dataset can be found at the provided link.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7VpgHnxtAhQ"
      },
      "source": [
        "## Three approaches\n",
        "\n",
        "In this task, you will apply three distinct NLP approaches to classify emotions from textual data. Each approach should be executable within the Google Colab environment, allowing you to leverage its resources."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AuYF84P1g6N"
      },
      "source": [
        "# 1. Train a RNN model\n",
        "\n",
        "* Introduction: Recurrent Neural Networks (RNNs) are powerful for sequence modeling and have been extensively used in NLP for tasks like text classification.\n",
        "* Task: Train a RNN to classify emotions.\n",
        "* Details: Implement and train an RNN using PyTorch. The architecture should include an embedding layer, one or more RNN layers, and a dense output layer for classification.\n",
        "* Model Flexibility: You are free to choose or modify any RNN architecture (e.g., LSTM, GRU) as long as it is compatible with Colab.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6D2fqAsw1weW"
      },
      "source": [
        "## Install dependency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-YUz9TL4SCJ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install datasets\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlldMx7914Mu"
      },
      "source": [
        "## Load and Prepare the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8kx3J97zuSc"
      },
      "outputs": [],
      "source": [
        "# Load and tokenize the dataset\n",
        "def load_and_preprocess_data():\n",
        "    dataset = load_dataset('dair-ai/emotion')\n",
        "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "    tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "    tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
        "    return tokenized_datasets, tokenizer\n",
        "\n",
        "tokenized_datasets, tokenizer = load_and_preprocess_data()\n",
        "print(tokenized_datasets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYGpJvEh2JHs"
      },
      "source": [
        "## Define the RNN Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EW91ZK-D5V69"
      },
      "outputs": [],
      "source": [
        "# Define the RNN Classifier\n",
        "class RNNClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, rnn_type=\"GRU\", num_layers=2, bidirectional=True, dropout=0.5):\n",
        "        super(RNNClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.rnn = nn.GRU(embedding_dim, hidden_dim, num_layers=num_layers,\n",
        "                          batch_first=True, bidirectional=bidirectional, dropout=dropout if num_layers > 1 else 0)\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.dropout(self.embedding(text))\n",
        "        output, _ = self.rnn(embedded)\n",
        "        hidden = output[:, -1, :]  # Get the last hidden state\n",
        "        return self.fc(hidden)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model and training parameters"
      ],
      "metadata": {
        "id": "twQZf-BikDTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model and training parameters\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "INPUT_DIM = tokenizer.vocab_size\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = len(tokenized_datasets['train'].features['label'].names)\n",
        "model = RNNClassifier(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "# Data loaders\n",
        "train_loader = DataLoader(tokenized_datasets['train'], batch_size=32, shuffle=True, pin_memory=True)\n",
        "validation_loader = DataLoader(tokenized_datasets['validation'], batch_size=32, shuffle=False, pin_memory=True)\n",
        "test_loader = DataLoader(tokenized_datasets['test'], batch_size=32, shuffle=False, pin_memory=True)"
      ],
      "metadata": {
        "id": "YkXGulKqkCiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbtVdYsx2Rii"
      },
      "source": [
        "## Training and evaluation functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmJbZr_96NWc"
      },
      "outputs": [],
      "source": [
        "def train_model(model, data_loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(data_loader):\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(batch['input_ids'])\n",
        "        loss = criterion(predictions, batch['label'])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(data_loader)\n",
        "\n",
        "def evaluate_model(model, data_loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            predictions = model(batch['input_ids'])\n",
        "            loss = criterion(predictions, batch['label'])\n",
        "            total_loss += loss.item()\n",
        "            preds = predictions.argmax(dim=1)\n",
        "            total_correct += (preds == batch['label']).sum().item()\n",
        "    avg_loss = total_loss / len(data_loader)\n",
        "    accuracy = total_correct / len(data_loader.dataset)\n",
        "    return avg_loss, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main training loop"
      ],
      "metadata": {
        "id": "1__5H4MwkZC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_model(model, train_loader, optimizer, criterion)\n",
        "    val_loss, val_accuracy = evaluate_model(model, validation_loader, criterion)\n",
        "    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
        "\n",
        "# Evaluation\n",
        "test_loss, test_accuracy = evaluate_model(model, test_loader, criterion)\n",
        "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')"
      ],
      "metadata": {
        "id": "L1hrFJXTkYpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVgGeAcC3Q-O"
      },
      "source": [
        "# 2. (**M8 Group Excercise**) Prompting a pretrained (transformer-based) language model\n",
        "\n",
        "* Introduction: Prompting involves adapting a pre-trained model to a specific task without extensive retraining, leveraging the model's existing knowledge.\n",
        "* Task: Use zero-shot learning by prompting a pretrained language model.\n",
        "* Details: Utilize a pre trained language model to generate predictions based on prompts. Craft three different prompts to evaluate how well the model can infer the correct emotion.\n",
        "* Model Flexibility: Any pretrained model available via libraries like Hugging Faceâ€™s Transformers that runs on Google Colab can be used.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5DMWwo75NIn"
      },
      "source": [
        "## Install dependency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSFa6hzP5O25"
      },
      "outputs": [],
      "source": [
        "# installation takes ~1 min\n",
        "!pip install -U sentence-transformers\n",
        "!pip install datasets\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import f1_score, accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8btt7GkC5Uc4"
      },
      "source": [
        "## Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJPgH4Ba4xW6"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset('dair-ai/emotion')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQqkpR4g5eDz"
      },
      "source": [
        "## Load the pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEtCUuta4_kV"
      },
      "outputs": [],
      "source": [
        "# distilbert model: https://arxiv.org/abs/1910.01108\n",
        "# many other models are avaliable on huggingface: https://huggingface.co/models\n",
        "from transformers import pipeline\n",
        "unmasker = pipeline('fill-mask', model='distilbert-base-uncased')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09KThRvn5hGo"
      },
      "source": [
        "## Prompt design"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trQjYmtN5GgF"
      },
      "outputs": [],
      "source": [
        "# prompt design\n",
        "unmasked = []\n",
        "##### Your implementation starts here #####\n",
        "prefix =  # string\n",
        "suffix =  # string\n",
        "##### Your implementation ends here #####"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test run"
      ],
      "metadata": {
        "id": "LxPV6dOsnA-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example = dataset['train']['text'][11]\n",
        "print(\"The raw data is:\\n\", example)\n",
        "prompt = example + prefix + '[MASK]' + suffix # [MASK] is the to-be-predicted token; defined by the model\n",
        "print(\"The prompt is:\\n\", prompt)\n",
        "pred = unmasker(prompt)\n",
        "print(\"\\nThe prediction is:\")\n",
        "(pred)"
      ],
      "metadata": {
        "id": "7N6v7VJjnAIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2PVN4_D5sbQ"
      },
      "source": [
        "## Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jl5zebV5JQ8m"
      },
      "outputs": [],
      "source": [
        "# mask filling\n",
        "for x in tqdm(dataset['test']['text'][0:100]): # Let's test only on first 100 data points for this coding exercise\n",
        "  prompt = x + prefix + '[MASK]' + suffix # distilbert\n",
        "\n",
        "  pred = unmasker(prompt) # this may take ~5 minutes to run on the entire dataset\n",
        "  unmasked.append(pred[0]['token_str'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzILPhTzGzOQ"
      },
      "outputs": [],
      "source": [
        "# sentence bert\n",
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('bert-base-nli-mean-tokens')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbFzEFSn5H1b"
      },
      "outputs": [],
      "source": [
        "prediction = []\n",
        "for i in tqdm(range(len(unmasked))):\n",
        "  z = unmasked[i]\n",
        "  x = dataset['test']['text'][i]\n",
        "  # predefine vocab\n",
        "  emotion = ['sadness', 'joy', 'love', 'anger', 'fear','shock']\n",
        "  emotion.append(z)\n",
        "\n",
        "  # put emotion into sentences\n",
        "  sentence = [x + prefix + z + suffix for z in emotion]\n",
        "\n",
        "  word = emotion\n",
        "\n",
        "  # get cosine similarity between sentences\n",
        "  sentence_embeddings = model.encode(word)\n",
        "  # back mapping\n",
        "  back_mapping = cosine_similarity(\n",
        "      [sentence_embeddings[6]],\n",
        "      sentence_embeddings[0:6]\n",
        "  )\n",
        "\n",
        "  prediction.append(np.argmax(back_mapping))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNCThezI5wdc"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlKIAh5_IInv"
      },
      "outputs": [],
      "source": [
        "label = dataset['test']['label'][0:len(unmasked)]\n",
        "print(len(prediction))\n",
        "print(len(label))\n",
        "\n",
        "# classes order: sadness, joy , love, anger, fear, surprise\n",
        "print('F1_macro: ', f1_score(prediction, label, average='macro'))\n",
        "print('F1: ', f1_score(prediction, label, average=None))\n",
        "print('Accuracy: ', accuracy_score(prediction, label))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UV8Yvuw3TxI"
      },
      "source": [
        "# 3. Fine-tune a pretrained (transformer-based) language model\n",
        "\n",
        "* Introduction: Fine-tuning adjusts the weights of a pretrained model specifically to the task at hand, improving performance by adapting the model's deep knowledge to your specific dataset.\n",
        "* Task: Fine-tune a pretrained model on the Emotion dataset.\n",
        "* Details: Choose a transformer model and fine-tune it using the training split of the Emotion dataset. Adjust the learning rate, batch size, and other hyperparameters as necessary.\n",
        "* Model Flexibility: Any transformer-based model that is supported by the Google Colab environment can be used. Ensure the chosen model is manageable within the resource constraints of Colab.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUPdO17h4ARD"
      },
      "source": [
        "## Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCcFHuf63_Hh"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset('dair-ai/emotion')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39KmN0ih4EzC"
      },
      "source": [
        "## Preprocess the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjcMdvEv4B8R"
      },
      "outputs": [],
      "source": [
        "# Load a tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Tokenize the function\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "# Apply the tokenizer to the dataset\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ-WJZW34Jvv"
      },
      "source": [
        "## Load the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCBtwOt44HTw"
      },
      "outputs": [],
      "source": [
        "from transformers import DistilBertForSequenceClassification\n",
        "\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=dataset['train'].features['label'].num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNw7gGUV4O2s"
      },
      "source": [
        "## Training config"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate -U # Note this package requires to restart runtime/session"
      ],
      "metadata": {
        "id": "Faumey_7ZsJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vt4DyYEG4Moa"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "# Set training arguments\n",
        "#In the following code, several key components have been removed. Fill in the blanks with the appropriate code to complete the script.\n",
        "# Sets the number of samples that will be processed at a time during training to be 16\n",
        "# Sets the number of samples that will be processed at a time during evaluation to be 64\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=???,\n",
        "    per_device_eval_batch_size=???,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"epoch\",\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=???,\n",
        "    args=???,\n",
        "    train_dataset=tokenized_datasets['train'],\n",
        "    eval_dataset=tokenized_datasets['validation'],\n",
        "    compute_metrics=lambda p: {'accuracy': (p.predictions.argmax(-1) == p.label_ids).astype(float).mean()}  # compute accuracy\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqDvg98L4YYz"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D20b5b2v4ZOK"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2L-jSCEX4b-0"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C86Tb1dE4WNC"
      },
      "outputs": [],
      "source": [
        "results = trainer.evaluate(tokenized_datasets['test'])\n",
        "print(results)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "1AuYF84P1g6N",
        "6D2fqAsw1weW",
        "hlldMx7914Mu",
        "DYGpJvEh2JHs",
        "twQZf-BikDTT",
        "jbtVdYsx2Rii"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}